{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9XLR/Gn7+KoT7IW1UhFDR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirthnaik/PersonalityPredictionAnalysis/blob/main/capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jN-jIpoP35M"
      },
      "outputs": [],
      "source": [
        "# Data Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "from numpy import loadtxt\n",
        "import pickle as pkl\n",
        "from scipy import sparse\n",
        "\n",
        "# Data Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Text Processing\n",
        "import re\n",
        "import itertools\n",
        "import string\n",
        "import collections\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Machine Learning packages\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import sklearn.cluster as cluster\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Model training and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "\n",
        "#Metrics\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Ignore noise warning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading dataset\n",
        "data_set = pd.read_csv(\"/content/sample_data/mbti_1.csv\")\n",
        "data_set.tail()"
      ],
      "metadata": {
        "id": "q1N1yP84QL08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set.isnull().any()"
      ],
      "metadata": {
        "id": "9VE-_TS2TKky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nRow, nCol = data_set.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns')"
      ],
      "metadata": {
        "id": "UncvBJP5TMoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set.dtypes"
      ],
      "metadata": {
        "id": "tuVyqnpjTPFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set.info()"
      ],
      "metadata": {
        "id": "K_bELdV-TRoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set.describe(include=['object'])"
      ],
      "metadata": {
        "id": "z4wiTk1yTT8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "types = np.unique(np.array(data_set['type']))\n",
        "types"
      ],
      "metadata": {
        "id": "mwri3-8LTWey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = data_set.groupby(['type']).count()*50\n",
        "total"
      ],
      "metadata": {
        "id": "cvyiLYQPTZKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,4))\n",
        "plt.bar(np.array(total.index), height = total['posts'],)\n",
        "plt.xlabel('Personality types', size = 14)\n",
        "plt.ylabel('No. of posts available', size = 14)\n",
        "plt.title('Total posts for each personality type')"
      ],
      "metadata": {
        "id": "30VTSBz4TdUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting this in descending order for better understanding of this visualization\n",
        "cnt_srs = data_set['type'].value_counts()\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.barplot(x=cnt_srs.index,y=cnt_srs.values, alpha=0.8)\n",
        "plt.xlabel('Personality types', fontsize=12)\n",
        "plt.ylabel('No. of posts availables', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I-RwmR48T0TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data_set.copy()\n",
        "#this function counts the no of words in each post of a user\n",
        "def var_row(row):\n",
        "    l = []\n",
        "    for i in row.split('|||'):\n",
        "        l.append(len(i.split()))\n",
        "    return np.var(l)\n",
        "\n",
        "#this function counts the no of words per post out of the total 50 posts in the whole row\n",
        "df['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\n",
        "df['variance_of_word_counts'] = df['posts'].apply(lambda x: var_row(x))\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.swarmplot(x=\"type\",y= \"words_per_comment\", data=df)"
      ],
      "metadata": {
        "id": "QFE7SsufT6hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "sns.jointplot(x=\"variance_of_word_counts\",y=\"words_per_comment\", data=df, kind=\"hex\")"
      ],
      "metadata": {
        "id": "rd6dIpMDUTc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_jointplot(mbti_type, axs, titles):\n",
        "    df_1 = df[df['type'] == mbti_type]\n",
        "    sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df_1, kind=\"hex\", ax = axs, title = titles)\n",
        "\n",
        "plt.figure(figsize=(24, 5))\n",
        "i = df['type'].unique()\n",
        "k = 0\n",
        "\n",
        "for m in range(1,3):\n",
        "  for n in range(1,7):\n",
        "    df_1 = df[df['type'] == i[k]]\n",
        "    sns.jointplot(x=\"variance_of_word_counts\",y=\"words_per_comment\", data=df_1, kind=\"hex\" )\n",
        "    plt.title(i[k])\n",
        "    k+=1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PyfwBZAbUaTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"length_posts\"] = df[\"posts\"].apply(len)\n",
        "sns.distplot(df[\"length_posts\"]).set_title(\"Distribution of Lengths of all 50 Posts\")"
      ],
      "metadata": {
        "id": "fAXi0xZcUjjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the most common words in all posts.\n",
        "words = list(df[\"posts\"].apply(lambda x: x.split()))\n",
        "words = [x for y in words for x in y]\n",
        "Counter(words).most_common(40)"
      ],
      "metadata": {
        "id": "MuXytxJ3UoJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the most common words with WordCloud.\n",
        "wc = wordcloud.WordCloud(width=1200, height=500,\n",
        "                         collocations=False, background_color=\"white\",\n",
        "                         colormap=\"tab20b\").generate(\" \".join(words))\n",
        "\n",
        "# collocations to False  is set to ensure that the word cloud doesn't appear as if it contains any duplicate words\n",
        "plt.figure(figsize=(25,10))\n",
        "# generate word cloud, interpolation\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "_ = plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "DqGj2OE_Ur2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(len(df['type'].unique()), sharex=True, figsize=(15,len(df['type'].unique())))\n",
        "k = 0\n",
        "for i in df['type'].unique():\n",
        "    df_4 = df[df['type'] == i]\n",
        "    wordcloud = WordCloud(max_words=1628,relative_scaling=1,normalize_plurals=False).generate(df_4['posts'].to_string())\n",
        "    plt.subplot(4,4,k+1)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(i)\n",
        "    ax[k].axis(\"off\")\n",
        "    k+=1"
      ],
      "metadata": {
        "id": "x5F_iyG6U1Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(posts, new_posts):\n",
        "    for post in posts[1].split(\"|||\"):\n",
        "        new_posts.append((posts[0], post))\n",
        "\n",
        "posts = []\n",
        "df.apply(lambda x: extract(x, posts), axis=1)\n",
        "print(\"Number of users\", len(df))\n",
        "print(\"Number of posts\", len(posts))\n",
        "print(\"5 posts from start are:\")\n",
        "posts[0:5]"
      ],
      "metadata": {
        "id": "h2jh5KQVU8L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(df, remove_special=True):\n",
        "    texts = df['posts'].copy()\n",
        "    labels = df['type'].copy()\n",
        "\n",
        "    #Remove links\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n",
        "\n",
        "    #Keep the End Of Sentence characters\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n",
        "\n",
        "    #Strip Punctation\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[\\.+]', \".\",x))\n",
        "\n",
        "    #Remove multiple fullstops\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
        "\n",
        "    #Remove Non-words\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n",
        "\n",
        "    #Convert posts to lowercase\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n",
        "\n",
        "    #Remove multiple letter repeating words\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x))\n",
        "\n",
        "    #Remove very short or long words\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x))\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n",
        "\n",
        "    #Remove MBTI Personality Words - crutial in order to get valid model accuracy estimation for unseen data.\n",
        "    if remove_special:\n",
        "        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n",
        "        pers_types = [p.lower() for p in pers_types]\n",
        "        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n",
        "\n",
        "    return df\n",
        "\n",
        "#Preprocessing of entered Text\n",
        "new_df = preprocess_text(data_set)"
      ],
      "metadata": {
        "id": "3FbrEBIuVDFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove posts with less than X words\n",
        "min_words = 15\n",
        "print(\"Before : Number of posts\", len(new_df))\n",
        "new_df[\"no. of. words\"] = new_df[\"posts\"].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
        "new_df = new_df[new_df[\"no. of. words\"] >= min_words]\n",
        "\n",
        "print(\"After : Number of posts\", len(new_df))"
      ],
      "metadata": {
        "id": "zD9_2De4VPjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "id": "XcfWjMe2VR9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting MBTI personality (or target or Y feature) into numerical form using Label Encoding\n",
        "# encoding personality type\n",
        "enc = LabelEncoder()\n",
        "new_df['type of encoding'] = enc.fit_transform(new_df['type'])\n",
        "\n",
        "target = new_df['type of encoding']"
      ],
      "metadata": {
        "id": "-odhZWxkVVYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(15)"
      ],
      "metadata": {
        "id": "AZuGwpVvVW_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# The python natural language toolkit library provides a list of english stop words.\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "XuQfQ92HVbdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the posts for the model and filtering Stop-words\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Converting posts (or training or X feature) into numerical form by count vectorization\n",
        "train =  vect.fit_transform(new_df[\"posts\"])"
      ],
      "metadata": {
        "id": "gDFEEFFGVh5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "ZEr7TOWvVoBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.4, stratify=target, random_state=42)\n",
        "print ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))"
      ],
      "metadata": {
        "id": "7bGQgsD0Vqwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = {}\n",
        "\n",
        "#Random Forest\n",
        "random_forest = RandomForestClassifier(n_estimators=100, random_state = 1)\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# make predictions for test data\n",
        "Y_pred = random_forest.predict(X_test)\n",
        "predictions = [round(value) for value in Y_pred]\n",
        "\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "accuracies['Random Forest'] = accuracy* 100.0\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "metadata": {
        "id": "mELnIhyAV3yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XG boost Classifier\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train,y_train)\n",
        "\n",
        "Y_pred = xgb.predict(X_test)\n",
        "predictions = [round(value) for value in Y_pred]\n",
        "\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "accuracies['XG Boost'] = accuracy* 100.0\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "metadata": {
        "id": "l4YrBGlsWGUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Descent\n",
        "sgd = SGDClassifier(max_iter=5, tol=None)\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "Y_pred = sgd.predict(X_test)\n",
        "predictions = [round(value) for value in Y_pred]\n",
        "\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "accuracies['Gradient Descent'] = accuracy* 100.0\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "metadata": {
        "id": "40Tl10t9X7Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "Y_pred = logreg.predict(X_test)\n",
        "predictions = [round(value) for value in Y_pred]\n",
        "\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "accuracies['Logistic Regression'] = accuracy* 100.0\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6IOn0tOYCa-",
        "outputId": "f32273bb-04fa-4383-9383-f0a69589f148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 58.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "Y_pred = knn.predict(X_test)\n",
        "predictions = [round(value) for value in Y_pred]\n",
        "\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "accuracies['KNN'] = accuracy* 100.0\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "\n",
        "#try to find best k value\n",
        "scoreList = []\n",
        "for i in range(1,20):\n",
        "    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n",
        "    knn2.fit(X_train, y_train)\n",
        "    scoreList.append(knn2.score(X_test, y_test))\n",
        "\n",
        "plt.plot(range(1,20), scoreList)\n",
        "plt.xticks(np.arange(1,20,1))\n",
        "plt.xlabel(\"K value\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n",
        "\n",
        "acc = max(scoreList)*100\n",
        "\n",
        "print(\"Maximum KNN Score is {:.2f}%\".format(acc))"
      ],
      "metadata": {
        "id": "nd7wggskYgcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC(random_state = 1)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "Y_pred = svm.predict(X_test)\n",
        "\n",
        "predictions = [round(value) for value in Y_pred]\n",
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "accuracies['SVM'] = accuracy* 100.0\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "metadata": {
        "id": "IubzPsJ3ZAk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.from_dict(accuracies, orient='index', columns=['Accuracies(%)'])"
      ],
      "metadata": {
        "id": "DSCIN1eTb1GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(16,5))\n",
        "plt.yticks(np.arange(0,100,10))\n",
        "plt.ylabel(\"Accuracy %\")\n",
        "plt.xlabel(\"Algorithms\")\n",
        "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZWorveRYb7gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-9l9yrQb9CU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}